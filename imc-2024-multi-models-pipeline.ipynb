{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2013b397",
   "metadata": {
    "papermill": {
     "duration": 0.013269,
     "end_time": "2024-06-04T00:08:52.399840",
     "exception": false,
     "start_time": "2024-06-04T00:08:52.386571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Updated information:\n",
    "- version4:\n",
    "  - Image matching methods\n",
    "     - added: MatchFormer\n",
    "     - added: SIFT + LightGlue\n",
    "     - added: DISK + LightGlue\n",
    "     - modified: Aliked + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: DogHardNet + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + SuperGlue ... added `torch.no_grad()` and speeding up by cuda cache of keypoints/descriptors\n",
    "  - Configuration\n",
    "     - added: CAMERA_MODEL = \"simple-radial\" or \"simple-pinhole\"\n",
    "     - added: ROTATION_CORRECTION ... `check_orientation` (LightGlue series only are supported. Others image matching methods are under construction.)\n",
    "         - https://github.com/ternaus/check_orientation\n",
    "     - added: DRY_RUN ... to run pipeline with only 10 images\n",
    "  - Pipeline\n",
    "     - Parallel execution of image matching and COLMAP processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b8e4c0",
   "metadata": {
    "papermill": {
     "duration": 0.012374,
     "end_time": "2024-06-04T00:08:52.424851",
     "exception": false,
     "start_time": "2024-06-04T00:08:52.412477",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14558121",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:08:52.450556Z",
     "iopub.status.busy": "2024-06-04T00:08:52.450258Z",
     "iopub.status.idle": "2024-06-04T00:10:06.537564Z",
     "shell.execute_reply": "2024-06-04T00:10:06.536325Z"
    },
    "papermill": {
     "duration": 74.103052,
     "end_time": "2024-06-04T00:10:06.540212",
     "exception": false,
     "start_time": "2024-06-04T00:08:52.437160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: pycolmap\r\n",
      "Successfully installed pycolmap-0.4.0\r\n",
      "Processing /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: safetensors\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.4.3\r\n",
      "    Uninstalling safetensors-0.4.3:\r\n",
      "      Successfully uninstalled safetensors-0.4.3\r\n",
      "Successfully installed safetensors-0.4.1\r\n",
      "\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0mProcessing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Installing collected packages: lightglue\r\n",
      "Successfully installed lightglue-0.0\r\n",
      "\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
    "!python -m pip install  --no-deps /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n",
    "\n",
    "# dkm\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dkm-dependencies/packages einops > /dev/null\n",
    "\n",
    "# match former\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/matchformer-dependencies yacs > /dev/null\n",
    "\n",
    "# lightglue models\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/pytorch-lightglue-models/* /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# dkm model\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/dkm-dependencies/DKMv3_outdoor.pth /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# check rotation\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/pkg-check-orientation/ check_orientation==0.0.5 > /dev/null\n",
    "!cp /kaggle/input/pkg-check-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe243d3",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:06.569859Z",
     "iopub.status.busy": "2024-06-04T00:10:06.569515Z",
     "iopub.status.idle": "2024-06-04T00:10:06.575387Z",
     "shell.execute_reply": "2024-06-04T00:10:06.574570Z"
    },
    "papermill": {
     "duration": 0.022455,
     "end_time": "2024-06-04T00:10:06.577336",
     "exception": false,
     "start_time": "2024-06-04T00:10:06.554881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6780177e",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:06.604955Z",
     "iopub.status.busy": "2024-06-04T00:10:06.604706Z",
     "iopub.status.idle": "2024-06-04T00:10:14.692828Z",
     "shell.execute_reply": "2024-06-04T00:10:14.692009Z"
    },
    "papermill": {
     "duration": 8.104575,
     "end_time": "2024-06-04T00:10:14.695141",
     "exception": false,
     "start_time": "2024-06-04T00:10:06.590566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# dkm\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/dkm-dependencies/DKM/')\n",
    "from dkm.utils.utils import tensor_to_pil, get_tuple_transform_ops\n",
    "from dkm import DKMv3_outdoor\n",
    "\n",
    "# LoFTR\n",
    "from kornia.feature import LoFTR\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint, DoGHardNet, LightGlue, DISK, SIFT\n",
    "from lightglue.utils import load_image, rbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9750120f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:14.723877Z",
     "iopub.status.busy": "2024-06-04T00:10:14.723250Z",
     "iopub.status.idle": "2024-06-04T00:10:14.727866Z",
     "shell.execute_reply": "2024-06-04T00:10:14.726992Z"
    },
    "papermill": {
     "duration": 0.021044,
     "end_time": "2024-06-04T00:10:14.730072",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.709028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.7.2\n",
      "Pycolmap version 0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4fc0c0",
   "metadata": {
    "papermill": {
     "duration": 0.013273,
     "end_time": "2024-06-04T00:10:14.756617",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.743344",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a3c3441",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:14.784596Z",
     "iopub.status.busy": "2024-06-04T00:10:14.784346Z",
     "iopub.status.idle": "2024-06-04T00:10:14.797596Z",
     "shell.execute_reply": "2024-06-04T00:10:14.796825Z"
    },
    "papermill": {
     "duration": 0.029724,
     "end_time": "2024-06-04T00:10:14.799597",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.769873",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        \n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = True\n",
    "    use_sift_lightglue = True\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = True\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 3072,\n",
    "        \"detection_threshold\" : 0.2,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 3072,\n",
    "        \"detection_threshold\" : 0.2,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 3072,\n",
    "        \"detection_threshold\" : 0.2,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.65,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 750,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27128cde",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:14.827379Z",
     "iopub.status.busy": "2024-06-04T00:10:14.827137Z",
     "iopub.status.idle": "2024-06-04T00:10:14.830760Z",
     "shell.execute_reply": "2024-06-04T00:10:14.829993Z"
    },
    "papermill": {
     "duration": 0.019918,
     "end_time": "2024-06-04T00:10:14.832754",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.812836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bc0d0",
   "metadata": {
    "papermill": {
     "duration": 0.013458,
     "end_time": "2024-06-04T00:10:14.859681",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.846223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COLMAP utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ad19d14",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:14.887944Z",
     "iopub.status.busy": "2024-06-04T00:10:14.887711Z",
     "iopub.status.idle": "2024-06-04T00:10:14.915462Z",
     "shell.execute_reply": "2024-06-04T00:10:14.914669Z"
    },
    "papermill": {
     "duration": 0.044401,
     "end_time": "2024-06-04T00:10:14.917342",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.872941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to manipulate a colmap database.\n",
    "# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "\n",
    "# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "#     * Redistributions of source code must retain the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer.\n",
    "#\n",
    "#     * Redistributions in binary form must reproduce the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer in the\n",
    "#       documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n",
    "#       its contributors may be used to endorse or promote products derived\n",
    "#       from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n",
    "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "# POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n",
    "\n",
    "# This script is based on an original implementation by True Price.\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f41e910",
   "metadata": {
    "papermill": {
     "duration": 0.013179,
     "end_time": "2024-06-04T00:10:14.943931",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.930752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# h5 to colmap db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74e615df",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:14.971722Z",
     "iopub.status.busy": "2024-06-04T00:10:14.971485Z",
     "iopub.status.idle": "2024-06-04T00:10:14.991377Z",
     "shell.execute_reply": "2024-06-04T00:10:14.990579Z"
    },
    "papermill": {
     "duration": 0.036046,
     "end_time": "2024-06-04T00:10:14.993209",
     "exception": false,
     "start_time": "2024-06-04T00:10:14.957163",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to interface DISK with Colmap.\n",
    "# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n",
    "\n",
    "#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, CONFIG.CAMERA_MODEL, single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05044b",
   "metadata": {
    "papermill": {
     "duration": 0.013159,
     "end_time": "2024-06-04T00:10:15.019531",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.006372",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Rotation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdf9ac94",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.047561Z",
     "iopub.status.busy": "2024-06-04T00:10:15.047322Z",
     "iopub.status.idle": "2024-06-04T00:10:15.517379Z",
     "shell.execute_reply": "2024-06-04T00:10:15.516369Z"
    },
    "papermill": {
     "duration": 0.487001,
     "end_time": "2024-06-04T00:10:15.519773",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.032772",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "from check_orientation.pre_trained_models import create_model\n",
    "\n",
    "def convert_rot_k(index):\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    elif index == 1:\n",
    "        return 3\n",
    "    elif index == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "class CheckRotationDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgPath = self.files[idx]\n",
    "        image = T_read_image(imgPath, mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def get_CheckRotation_dataloader(images, batch_size=1):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    dataset = CheckRotationDataset(images, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def exec_rotation_detection(img_files, device):\n",
    "    model = create_model(\"swsl_resnext50_32x4d\")\n",
    "    model.eval().to(device);\n",
    "    \n",
    "    dataloader = get_CheckRotation_dataloader(img_files)\n",
    "    \n",
    "    rots = []\n",
    "    for idx, image in enumerate(dataloader):\n",
    "        image = image.to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image).detach().cpu().numpy()\n",
    "            detected_rot = prediction[0].argmax()\n",
    "            rot_k = convert_rot_k(detected_rot)\n",
    "            rots.append(rot_k)\n",
    "            print(f\"{os.path.basename(img_files[idx])} > rot_k={rot_k}\")\n",
    "    return rots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7945b86",
   "metadata": {
    "papermill": {
     "duration": 0.013355,
     "end_time": "2024-06-04T00:10:15.547186",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.533831",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0985c0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.575411Z",
     "iopub.status.busy": "2024-06-04T00:10:15.575067Z",
     "iopub.status.idle": "2024-06-04T00:10:15.592635Z",
     "shell.execute_reply": "2024-06-04T00:10:15.591897Z"
    },
    "papermill": {
     "duration": 0.033776,
     "end_time": "2024-06-04T00:10:15.594448",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.560672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, model,\n",
    "                    device =  torch.device('cpu')):\n",
    "    model = model.eval()\n",
    "    model= model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all.to(torch.float32)\n",
    "\n",
    "def convert_1d_to_2d(idx, num_images):\n",
    "    idx1 = idx // num_images\n",
    "    idx2 = idx % num_images\n",
    "    return (idx1, idx2)\n",
    "\n",
    "def get_pairs_from_distancematrix(mat):\n",
    "    pairs = [ convert_1d_to_2d(idx, mat.shape[0]) for idx in np.argsort(mat.flatten())]\n",
    "    pairs = [ pair for pair in pairs if pair[0] < pair[1] ]\n",
    "    return pairs\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames, model, device):\n",
    "    #index_pairs = []\n",
    "    #for i in range(len(img_fnames)):\n",
    "    #    for j in range(i+1, len(img_fnames)):\n",
    "    #        index_pairs.append((i,j))\n",
    "    #return index_pairs\n",
    "    descs = get_global_desc(img_fnames, model, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    matching_list = get_pairs_from_distancematrix(dm)\n",
    "    return matching_list\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    model = timm.create_model('tf_efficientnet_b7',\n",
    "                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    model.eval()\n",
    "    descs = get_global_desc(fnames, model, device=device)\n",
    "\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames, model, device)\n",
    "    \n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8aa10b",
   "metadata": {
    "papermill": {
     "duration": 0.013343,
     "end_time": "2024-06-04T00:10:15.621055",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.607712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LightGlue series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1672961",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.649267Z",
     "iopub.status.busy": "2024-06-04T00:10:15.648963Z",
     "iopub.status.idle": "2024-06-04T00:10:15.672590Z",
     "shell.execute_reply": "2024-06-04T00:10:15.671804Z"
    },
    "papermill": {
     "duration": 0.040118,
     "end_time": "2024-06-04T00:10:15.674454",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.634336",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=True\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    \n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features, detection_threshold=detection_threshold #, resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "        \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95faa97d",
   "metadata": {
    "papermill": {
     "duration": 0.013151,
     "end_time": "2024-06-04T00:10:15.701005",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.687854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31d64b60",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.728984Z",
     "iopub.status.busy": "2024-06-04T00:10:15.728732Z",
     "iopub.status.idle": "2024-06-04T00:10:15.797401Z",
     "shell.execute_reply": "2024-06-04T00:10:15.796690Z"
    },
    "papermill": {
     "duration": 0.085127,
     "end_time": "2024-06-04T00:10:15.799357",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.714230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.superpoint import SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          process_resize, frame2tensor,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "from torch.nn import functional as torchF  # For resizing tensor\n",
    "\n",
    "def sg_imread(path):\n",
    "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "# Preprocess\n",
    "def sg_read_image(image, device, resize):\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, [resize,])\n",
    "    \n",
    "    unit_shape = 8\n",
    "    w_new = w_new // unit_shape * unit_shape\n",
    "    h_new = h_new // unit_shape * unit_shape\n",
    "    \n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "\n",
    "    inp = frame2tensor(image, \"cpu\")\n",
    "    return image, inp, scales, (h, w)\n",
    "\n",
    "class SGDataset(Dataset):\n",
    "    def __init__(self, img_fnames, resize_to, device):\n",
    "        self.img_fnames = img_fnames\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.img_fnames[idx]\n",
    "        im = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        _, image, scale, ori_shape = sg_read_image(im, self.device, self.resize_to)\n",
    "        return image, torch.tensor([idx]), torch.tensor(ori_shape)\n",
    "\n",
    "def get_superglue_dataloader(img_fnames, resize_to, device, batch_size=1):\n",
    "    dataset = SGDataset(img_fnames, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def detect_superglue(\n",
    "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints, \n",
    "    resize_to=750, min_matches=15\n",
    "):    \n",
    "    t=time()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    dataloader = get_superglue_dataloader( img_fnames, resize_to, device)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    superpoint = SuperPoint(sg_config[\"superpoint\"]).eval().to(device)\n",
    "    dict_features_cuda = {}\n",
    "    dict_shapes = {}\n",
    "    dict_images = {}\n",
    "    for X in dataloader:\n",
    "        image, idx, ori_shape = X\n",
    "        image = image[0].to(device)\n",
    "        fname = img_fnames[idx]\n",
    "        key = fname.split('/')[-1]\n",
    "        \n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            pred = superpoint({'image': image})\n",
    "            dict_features_cuda[key] = pred\n",
    "            dict_shapes[key] = ori_shape\n",
    "            dict_images[key] = image.half()\n",
    "    del superpoint\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    superglue = SuperGlue(sg_config[\"superglue\"]).eval().to(device)\n",
    "    weights = sg_config[\"superglue\"][\"weights\"]\n",
    "    cnt_pairs = 0\n",
    "    \n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx, (fname1, fname2) in enumerate(zip(fnames1, fnames2)):\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            data = {\"image0\": dict_images[key1], \"image1\": dict_images[key2]}\n",
    "            data = {**data, **{k+'0': v for k, v in dict_features_cuda[key1].items()}}\n",
    "            data = {**data, **{k+'1': v for k, v in dict_features_cuda[key2].items()}}\n",
    "            for k in data:\n",
    "                if isinstance(data[k], (list, tuple)):\n",
    "                    data[k] = torch.stack(data[k])\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = {**data, **superglue(data)}\n",
    "                pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
    "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "\n",
    "            valid = matches > -1\n",
    "            mkpts1 = mkpts1[valid]\n",
    "            mkpts2 = mkpts2[matches[valid]]\n",
    "            mconf = conf[valid]\n",
    "\n",
    "            ori_shape_1 = dict_shapes[key1][0].numpy()\n",
    "            ori_shape_2 = dict_shapes[key2][0].numpy()\n",
    "            \n",
    "            # Scaling coords\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / dict_images[key1].shape[3]   # X\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / dict_images[key1].shape[2]   # Y\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / dict_images[key2].shape[3]   # X\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / dict_images[key2].shape[2]   # Y  \n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue/{resize_to}/{weights})')            \n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')            \n",
    "\n",
    "    del superglue\n",
    "    del dict_features_cuda\n",
    "    del dict_images\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c8ec1e",
   "metadata": {
    "papermill": {
     "duration": 0.013239,
     "end_time": "2024-06-04T00:10:15.826067",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.812828",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "175a5b4d",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.854597Z",
     "iopub.status.busy": "2024-06-04T00:10:15.854345Z",
     "iopub.status.idle": "2024-06-04T00:10:15.881878Z",
     "shell.execute_reply": "2024-06-04T00:10:15.881044Z"
    },
    "papermill": {
     "duration": 0.043895,
     "end_time": "2024-06-04T00:10:15.883709",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.839814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "    print(\"***\", dense_matches.shape, dense_certainty.shape)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "                #print(\"---\", mconf.shape)\n",
    "                if mconf.shape[0] > 3000:\n",
    "                    rand_idx = np.random.choice(range(mconf.shape[0]), 3000, replace=False)\n",
    "                    mkpts1 = mkpts1[rand_idx, :]\n",
    "                    mkpts2 = mkpts2[rand_idx, :]\n",
    "                    mconf  = mconf[rand_idx]\n",
    "            except:\n",
    "                mkpts1 = np.empty((0,2))\n",
    "                mkpts2 = np.empty((0,2))\n",
    "                mconf = np.empty((0,))\n",
    "        \n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, \n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(f'{feature_dir}/matches_dkm.h5', mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')            \n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9cac31",
   "metadata": {
    "papermill": {
     "duration": 0.013304,
     "end_time": "2024-06-04T00:10:15.910324",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.897020",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LoFTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf119bef",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:15.938401Z",
     "iopub.status.busy": "2024-06-04T00:10:15.938146Z",
     "iopub.status.idle": "2024-06-04T00:10:15.959789Z",
     "shell.execute_reply": "2024-06-04T00:10:15.958973Z"
    },
    "papermill": {
     "duration": 0.03805,
     "end_time": "2024-06-04T00:10:15.961662",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.923612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoFTRDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_small_edge_to = resize_small_edge_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        ratio = self.resize_small_edge_to / min([img.shape[0], img.shape[1]])\n",
    "        w = int(img.shape[1] * ratio) # int( (img.shape[1] * ratio) // self.round_unit * self.round_unit )\n",
    "        h = int(img.shape[0] * ratio) # int( (img.shape[0] * ratio) // self.round_unit * self.round_unit )\n",
    "        img_resized = cv2.resize(img, (w, h))\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_loftr_dataloader(images1, images2, idxs1, idxs2, resize_small_edge_to, device, batch_size=1):\n",
    "    dataset = LoFTRDataset(images1, images2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_loftr(img_fnames, index_pairs, feature_dir, device, file_keypoints, resize_small_edge_to=750, min_matches=15):\n",
    "    t=time()\n",
    "\n",
    "    matcher = LoFTR(pretrained=None)\n",
    "    matcher.load_state_dict(torch.load(\"../input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt\")['state_dict'])\n",
    "    matcher = matcher.to(device).eval()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "        \n",
    "    dataloader = get_loftr_dataloader( fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "\n",
    "    cnt_pairs = 0\n",
    "\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        store_mkpts = {}\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                correspondences = matcher( {\"image0\": image1.to(device),\"image1\": image2.to(device)} )\n",
    "                mkpts1 = correspondences['keypoints0'].cpu().numpy()\n",
    "                mkpts2 = correspondences['keypoints1'].cpu().numpy()\n",
    "                mconf  = correspondences['confidence'].cpu().numpy()\n",
    "\n",
    "            mkpts1[:,0] *= (float(ori_shape_1[1]) / float(image1.shape[3]))\n",
    "            mkpts1[:,1] *= (float(ori_shape_1[0]) / float(image1.shape[2]))\n",
    "\n",
    "            mkpts2[:,0] *= (float(ori_shape_2[1]) / float(image2.shape[3]))\n",
    "            mkpts2[:,1] *= (float(ori_shape_2[0]) / float(image2.shape[2]))\n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(loftr)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfd93f5",
   "metadata": {
    "papermill": {
     "duration": 0.013267,
     "end_time": "2024-06-04T00:10:15.988457",
     "exception": false,
     "start_time": "2024-06-04T00:10:15.975190",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df11378",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.016823Z",
     "iopub.status.busy": "2024-06-04T00:10:16.016572Z",
     "iopub.status.idle": "2024-06-04T00:10:16.041823Z",
     "shell.execute_reply": "2024-06-04T00:10:16.041124Z"
    },
    "papermill": {
     "duration": 0.041755,
     "end_time": "2024-06-04T00:10:16.043646",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.001891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "            except:\n",
    "                pass\n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')\n",
    "                else:\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96825f9",
   "metadata": {
    "papermill": {
     "duration": 0.014522,
     "end_time": "2024-06-04T00:10:16.071657",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.057135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: MatchFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b95bc098",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.099986Z",
     "iopub.status.busy": "2024-06-04T00:10:16.099713Z",
     "iopub.status.idle": "2024-06-04T00:10:16.124977Z",
     "shell.execute_reply": "2024-06-04T00:10:16.124150Z"
    },
    "papermill": {
     "duration": 0.041671,
     "end_time": "2024-06-04T00:10:16.126811",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.085140",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MatchFormerDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        #ratio = self.resize_long_edge_to / max([img.shape[0], img.shape[1]])\n",
    "        #w = int(img.shape[1] * ratio)\n",
    "        #h = int(img.shape[0] * ratio)\n",
    "        img_resized = cv2.resize(img, self.resize_to)\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_matchformer_dataloader(images1, images2, idxs1, idxs2, resize_to, device, batch_size=1):\n",
    "    dataset = MatchFormerDataset(images1, images2, idxs1, idxs2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_matchformer(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(560, 750), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "\n",
    "    sys.path.append('/kaggle/input/matchformer/MatchFormer-main')\n",
    "\n",
    "    from yacs.config import CfgNode as CN\n",
    "    from model.matchformer import Matchformer\n",
    "    from config import defaultmf\n",
    "\n",
    "    cfg = defaultmf.get_cfg_defaults()\n",
    "    cfg.MATCHFORMER.BACKBONE_TYPE = 'largela'\n",
    "    cfg.MATCHFORMER.SCENS = 'outdoor'\n",
    "    cfg.MATCHFORMER.RESOLUTION = (8,2)\n",
    "    cfg.MATCHFORMER.MATCH_COARSE.THR = detection_threshold\n",
    "\n",
    "    def lower_config(yacs_cfg):\n",
    "        if not isinstance(yacs_cfg, CN):\n",
    "            return yacs_cfg\n",
    "        return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n",
    "\n",
    "    _cfg = lower_config(cfg)\n",
    "\n",
    "    matcher_mf = Matchformer(_cfg['matchformer'])\n",
    "\n",
    "    pretrained_ckpt = '/kaggle/input/matchformer/outdoor-large-LA.ckpt'\n",
    "    matcher_mf.load_state_dict({k.replace('matcher.',''):v  for k,v in torch.load(pretrained_ckpt, map_location='cpu').items()})\n",
    "    matcher_mf = matcher_mf.to(device).eval()\n",
    "    \n",
    "    \n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_matchformer_dataloader(fnames1, fnames2, idxs1, idxs2, resize_to, device, batch_size=1)\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            #print(image1.shape, image2.shape)\n",
    "            input_dict = {\n",
    "                \"image0\": image1, \n",
    "                \"image1\": image2\n",
    "            }\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                matcher_mf(input_dict)\n",
    "\n",
    "            conf = input_dict['mconf'].to('cpu').numpy()\n",
    "            mkpts1 = input_dict['mkpts0_f'].to('cpu').numpy()\n",
    "            mkpts2 = input_dict['mkpts1_f'].to('cpu').numpy()\n",
    "\n",
    "            sorted_idx = np.argsort(-conf)\n",
    "            if len(conf) > num_features:\n",
    "                mkpts1 = mkpts1[sorted_idx[:num_features], :]\n",
    "                mkpts2 = mkpts2[sorted_idx[:num_features], :]\n",
    "\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / image1.shape[3]\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / image1.shape[2]\n",
    "\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / image2.shape[3]\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / image2.shape[2]\n",
    "                \n",
    "            n_matches = mkpts1.shape[0]\n",
    "\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(MatchFormer)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61dd3d0",
   "metadata": {
    "papermill": {
     "duration": 0.013154,
     "end_time": "2024-06-04T00:10:16.153669",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.140515",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50a152aa",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.182239Z",
     "iopub.status.busy": "2024-06-04T00:10:16.181980Z",
     "iopub.status.idle": "2024-06-04T00:10:16.229519Z",
     "shell.execute_reply": "2024-06-04T00:10:16.228875Z"
    },
    "papermill": {
     "duration": 0.064062,
     "end_time": "2024-06-04T00:10:16.231254",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.167192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf2fb01",
   "metadata": {
    "papermill": {
     "duration": 0.013315,
     "end_time": "2024-06-04T00:10:16.257924",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.244609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd987d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.285953Z",
     "iopub.status.busy": "2024-06-04T00:10:16.285695Z",
     "iopub.status.idle": "2024-06-04T00:10:16.318773Z",
     "shell.execute_reply": "2024-06-04T00:10:16.317968Z"
    },
    "papermill": {
     "duration": 0.04941,
     "end_time": "2024-06-04T00:10:16.320626",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.271216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f2ff0",
   "metadata": {
    "papermill": {
     "duration": 0.013207,
     "end_time": "2024-06-04T00:10:16.347252",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.334045",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reconstruction wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7f0894a",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.375387Z",
     "iopub.status.busy": "2024-06-04T00:10:16.375137Z",
     "iopub.status.idle": "2024-06-04T00:10:16.388524Z",
     "shell.execute_reply": "2024-06-04T00:10:16.387690Z"
    },
    "papermill": {
     "duration": 0.029551,
     "end_time": "2024-06-04T00:10:16.390333",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.360782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(dataset, scene, feature_dir, img_dir, timings, image_paths):\n",
    "    scene_result = {}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    options = pycolmap.SiftMatchingOptions()\n",
    "    options.confidence = 0.9999\n",
    "    options.max_num_trials = 20000\n",
    "    pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    t=time() - t \n",
    "    timings['RANSAC'].append(t)\n",
    "    print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "    mapper_options.min_model_size = 3\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    clear_output(wait=False)\n",
    "    t=time() - t\n",
    "    timings['Reconstruction'].append(t)\n",
    "    print(f'Reconstruction done in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    imgs_registered  = 0\n",
    "    best_idx = None\n",
    "    list_num_images = []            \n",
    "    print (\"Looking for the best reconstruction\")\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            print (idx1, rec.summary())\n",
    "            list_num_images.append( len(rec.images) )\n",
    "            if len(rec.images) > imgs_registered:\n",
    "                imgs_registered = len(rec.images)\n",
    "                best_idx = idx1\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if best_idx is not None:\n",
    "        print (maps[best_idx].summary())\n",
    "        for k, im in maps[best_idx].images.items():\n",
    "            key1 = f'test/{dataset}/images/{im.name}'\n",
    "            scene_result[key1] = {}\n",
    "            scene_result[key1][\"R\"] = deepcopy(im.rotmat())\n",
    "            scene_result[key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
    "\n",
    "    print(f'Registered: {dataset} / {scene} -> {len(scene_result)} images')\n",
    "    print(f'Total: {dataset} / {scene} -> {len(image_paths)} images')\n",
    "    print(timings)\n",
    "    return scene_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03db1ea6",
   "metadata": {
    "papermill": {
     "duration": 0.013414,
     "end_time": "2024-06-04T00:10:16.417049",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.403635",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c12604d0",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.444802Z",
     "iopub.status.busy": "2024-06-04T00:10:16.444536Z",
     "iopub.status.idle": "2024-06-04T00:10:16.452683Z",
     "shell.execute_reply": "2024-06-04T00:10:16.451887Z"
    },
    "papermill": {
     "duration": 0.024139,
     "end_time": "2024-06-04T00:10:16.454543",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.430404",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9c5798",
   "metadata": {
    "papermill": {
     "duration": 0.053358,
     "end_time": "2024-06-04T00:10:16.521174",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.467816",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "369cb2af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.550086Z",
     "iopub.status.busy": "2024-06-04T00:10:16.549742Z",
     "iopub.status.idle": "2024-06-04T00:10:16.561368Z",
     "shell.execute_reply": "2024-06-04T00:10:16.560469Z"
    },
    "papermill": {
     "duration": 0.028736,
     "end_time": "2024-06-04T00:10:16.563352",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.534616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church / church -> 41 images\n"
     ]
    }
   ],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# Get data from csv.\n",
    "data_dict = {}\n",
    "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            image, dataset, scene, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)\n",
    "            \n",
    "            if CONFIG.DRY_RUN:\n",
    "                if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "                    break\n",
    "                    \n",
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a881ab5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:10:16.591695Z",
     "iopub.status.busy": "2024-06-04T00:10:16.591440Z",
     "iopub.status.idle": "2024-06-04T00:28:19.512702Z",
     "shell.execute_reply": "2024-06-04T00:28:19.511617Z"
    },
    "papermill": {
     "duration": 1082.938295,
     "end_time": "2024-06-04T00:28:19.515230",
     "exception": false,
     "start_time": "2024-06-04T00:10:16.576935",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction done in  374.4427 sec\n",
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 33373\n",
      "\tnum_observations = 145494\n",
      "\tmean_track_length = 4.35963\n",
      "\tmean_observations_per_image = 3828.79\n",
      "\tmean_reprojection_error = 1.05929\n",
      "list_num_images = [38]\n",
      "Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 33373\n",
      "\tnum_observations = 145494\n",
      "\tmean_track_length = 4.35963\n",
      "\tmean_observations_per_image = 3828.79\n",
      "\tmean_reprojection_error = 1.05929\n",
      "Registered: church / church -> 38 images\n",
      "Total: church / church -> 41 images\n",
      "{'rotation_detection': [5.245208740234375e-06], 'shortlisting': [13.133011817932129], 'feature_detection': [], 'feature_matching': [114.50283885002136, 122.55593585968018, 32.44036555290222, 376.3537015914917], 'RANSAC': [37.221975803375244], 'Reconstruction': [374.4426634311676]}\n",
      "  => Merged observations: 25\n",
      "  => Filtered observations: 3\n",
      "  => Changed observations: 0.000487\n",
      "  => Filtered images: 2\n",
      "\n",
      "==============================================================================\n",
      "Finding good initial image pair\n",
      "==============================================================================\n",
      "\n",
      "  => No good initial image pair found.\n",
      "\n",
      "Elapsed time: 6.240 [minutes]\n",
      "test/church/images/00046.png\n",
      "test/church/images/00090.png\n",
      "test/church/images/00092.png\n",
      "test/church/images/00087.png\n",
      "test/church/images/00050.png\n",
      "test/church/images/00068.png\n",
      "test/church/images/00083.png\n",
      "test/church/images/00096.png\n",
      "test/church/images/00069.png\n",
      "test/church/images/00081.png\n",
      "test/church/images/00042.png\n",
      "test/church/images/00018.png\n",
      "test/church/images/00030.png\n",
      "test/church/images/00024.png\n",
      "test/church/images/00032.png\n",
      "test/church/images/00026.png\n",
      "test/church/images/00037.png\n",
      "test/church/images/00008.png\n",
      "test/church/images/00035.png\n",
      "test/church/images/00021.png\n",
      "test/church/images/00010.png\n",
      "test/church/images/00039.png\n",
      "test/church/images/00011.png\n",
      "test/church/images/00013.png\n",
      "test/church/images/00006.png\n",
      "test/church/images/00012.png\n",
      "test/church/images/00029.png\n",
      "test/church/images/00001.png\n",
      "test/church/images/00072.png\n",
      "test/church/images/00066.png\n",
      "test/church/images/00058.png\n",
      "test/church/images/00059.png\n",
      "test/church/images/00111.png\n",
      "test/church/images/00061.png\n",
      "test/church/images/00060.png\n",
      "test/church/images/00074.png\n",
      "test/church/images/00076.png\n",
      "test/church/images/00063.png\n"
     ]
    }
   ],
   "source": [
    "out_results = {}\n",
    "timings = {\n",
    "    \"rotation_detection\" : [],\n",
    "    \"shortlisting\":[],\n",
    "   \"feature_detection\": [],\n",
    "   \"feature_matching\":[],\n",
    "   \"RANSAC\": [],\n",
    "   \"Reconstruction\": []\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    futures = defaultdict(dict)\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        if dataset not in out_results:\n",
    "            out_results[dataset] = {}\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(scene)\n",
    "            # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "            # You may want to run this on the training data in that case?\n",
    "            img_dir = f'{src}/test/{dataset}/images'\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            if CONFIG.ROTATION_CORRECTION:\n",
    "                rots = exec_rotation_detection(img_fnames, device)\n",
    "            else:\n",
    "                rots = [ 0 for fname in img_fnames ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print (f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            t=time()\n",
    "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                                  sim_th = 0.3, # should be strict\n",
    "                                  min_pairs = 50, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                                  exhaustive_if_less = 50,\n",
    "                                  device=device)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################            \n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            futures[dataset][scene] = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene])\n",
    "                \n",
    "    #############################################################\n",
    "    # reconstruction results\n",
    "    #############################################################            \n",
    "    for dataset in datasets:\n",
    "        for scene in data_dict[dataset]:\n",
    "            # wait to complete COLMAP reconstruction\n",
    "            result = futures[dataset][scene].result()\n",
    "            if result is not None:\n",
    "                out_results[dataset][scene] = result   # get R and t from result\n",
    "    \n",
    "    create_submission(out_results, data_dict)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3007c3e6",
   "metadata": {
    "papermill": {
     "duration": 0.015085,
     "end_time": "2024-06-04T00:28:19.546655",
     "exception": false,
     "start_time": "2024-06-04T00:28:19.531570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "320b2860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-04T00:28:19.578030Z",
     "iopub.status.busy": "2024-06-04T00:28:19.577707Z",
     "iopub.status.idle": "2024-06-04T00:28:20.546912Z",
     "shell.execute_reply": "2024-06-04T00:28:20.545836Z"
    },
    "papermill": {
     "duration": 0.98753,
     "end_time": "2024-06-04T00:28:20.549164",
     "exception": false,
     "start_time": "2024-06-04T00:28:19.561634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path,dataset,scene,rotation_matrix,translation_vector\r\n",
      "test/church/images/00046.png,church,church,-0.19648469868486496;-0.320037262844815;-0.9268062977631931;0.37759094975667123;0.8476342624093107;-0.3727482150885216;0.9048860910880291;-0.4231929909398578;-0.0457039886093622,5.341916897855394;4.443658586918391;10.041523380678418\r\n",
      "test/church/images/00090.png,church,church,0.9993904120972498;-0.00355867441153693;-0.03472952698385029;-0.002238159590881557;0.9862136672098586;-0.1654617576666222;0.03483955869030286;0.16543862440468649;0.9856045184075355,0.060097839650861455;-0.0038980562698058005;-0.8433394090455908\r\n",
      "test/church/images/00092.png,church,church,0.9457896863625694;-0.11985158372577301;-0.3018566995258124;0.14354387480378797;0.9879729995360013;0.05748485186682621;0.29133661832407376;-0.09769826030314217;0.9516186340952096,0.24802862360416253;-0.136382751347819;-0.760294010560264\r\n",
      "test/church/images/00087.png,church,church,0.8900470340471613;-0.15722745435697313;-0.42789695579692666;0.18270253107818138;0.982987728286372;0.01883908639036949;0.4176554349239062;-0.09494552983065382;0.9036311659320406,0.9223872174850745;-0.08112302003660057;-0.7943029802187734\r\n",
      "test/church/images/00050.png,church,church,0.925412169678942;0.17354533610282563;0.3368891991843466;-0.2026351411827494;0.9778239671750576;0.05290830535822457;-0.32023634360884196;-0.11722758009525405;0.9400565827103443,-2.65754909132894;0.2853228356168893;0.6243426482270642\r\n",
      "test/church/images/00068.png,church,church,0.5686947355440142;-0.2060696278545998;-0.7963175285276537;0.526521803436584;0.834980710322998;0.15994375228307292;0.6319502261020334;-0.5102377111350831;0.583349286333,4.580113669293907;0.3041811940936563;0.558975048732269\r\n",
      "test/church/images/00083.png,church,church,0.9324215780010614;-0.14373026990689614;-0.331559361789863;0.1783358482494476;0.9810103909848141;0.07625574082632862;0.3143029209332304;-0.13023141822276602;0.940347516400467,1.4775140783878504;-0.11972536591471329;-0.772538071688418\r\n",
      "test/church/images/00096.png,church,church,0.9448507035192972;0.08928993224356267;0.3150943605636714;-0.015203236350781149;0.9730373875217678;-0.23014583222225724;-0.3271482991909805;0.21266299745078618;0.9207325560941655,-0.9540565409173805;-0.0004555073440987793;-1.2037450196295503\r\n",
      "test/church/images/00069.png,church,church,0.5818318363614348;-0.20314972582735064;-0.7875289855570733;0.34610833251969925;0.9380942601863904;0.013717914046315782;0.735989630604069;-0.28055186312270514;0.6161249189423075,4.600128960107019;0.13812285694987764;0.5687930527576139\r\n",
      "test/church/images/00081.png,church,church,0.8866312165349255;-0.12977872110655164;-0.44389477290653695;0.16013252171974424;0.9865958990254968;0.03140234885716852;0.4338694058740185;-0.0989242921330224;0.8955285160577002,1.5558362718591279;-0.046320999915457045;-0.5037250013613408\r\n",
      "test/church/images/00042.png,church,church,0.7310601502371226;0.18899748647493272;0.6556149836919786;-0.24791085206596045;0.9687786999017606;-0.00283549723751457;-0.6356817333880259;-0.1604611501978348;0.7550900298070309,-4.557787197105866;2.228399427412471;6.296114316870642\r\n",
      "test/church/images/00018.png,church,church,0.9833057278456545;-0.05859873767584655;-0.17226733157718804;0.08373585746351654;0.9862490441386359;0.14248203083363414;0.16154922395597315;-0.15452834975636137;0.9746911497293891,0.3459310236713032;1.010176166577126;2.4088604848351824\r\n",
      "test/church/images/00030.png,church,church,0.9485285542617394;0.07234331584477874;0.308318060454974;-0.15338295129994403;0.956696122121938;0.24739886856524818;-0.2770690383291742;-0.28195562517784345;0.9185498208752305,-2.5728156264112365;0.0467261971775747;0.4064976162304737\r\n",
      "test/church/images/00024.png,church,church,0.8169567993849041;0.20895271666249712;0.537513116247535;-0.22858286342657277;0.9730358951865226;-0.030838632042804966;-0.5294633721859167;-0.09767245711010308;0.8426913009136814,-0.10703591069144731;0.4231130327765668;1.6249792332348025\r\n",
      "test/church/images/00032.png,church,church,0.9760034258081126;0.08215309978393448;0.20166353415210903;-0.10424626497868467;0.9893602135496077;0.10148440315175764;-0.19118061891525984;-0.12007179535996584;0.9741831115915595,-0.9552691517938562;0.39297801412004274;1.2107178286460893\r\n",
      "test/church/images/00026.png,church,church,0.9662140920488969;0.13723144186172093;0.21816933719173023;-0.18177322022542036;0.9629252723219967;0.19933242669593282;-0.1827260921023777;-0.23225514265170039;0.9553369688109173,-1.9472238125051964;0.4063010394764341;0.6912828448039678\r\n",
      "test/church/images/00037.png,church,church,-0.78041027485382;0.19871990826068653;0.5928492227906261;-0.08266755370430912;0.9070372361052369;-0.4128553352969027;-0.6197788948222829;-0.3712059406575238;-0.6914334900432815,-0.7693173968875202;0.41201434119920655;3.368009351720978\r\n",
      "test/church/images/00008.png,church,church,0.999999816646074;0.00029570693624242497;0.0005284555103244701;-0.0002956956960530691;0.9999999560541605;-2.1347892136064463e-05;-0.0005284617998208279;2.119162620188615e-05;0.9999998601395108,-0.6767094564642209;1.2127093742183404;4.549340835755322\r\n",
      "test/church/images/00035.png,church,church,-0.4594588971203828;0.20748780711017561;0.8636239527465256;-0.3512753502421015;0.8506080410448996;-0.39124364381041066;-0.8157837643451918;-0.48313017956899473;-0.3179340803066264,-0.8710222533544272;1.0943872033666813;3.2072086883592266\r\n",
      "test/church/images/00021.png,church,church,0.9715691646411659;0.13667160624660377;0.19332415877080703;-0.15175848158366026;0.9862487583667693;0.06544272218839767;-0.18172154959137002;-0.09292071171692154;0.9789499577343737,-1.637231350081202;0.6077147829841069;1.5961690869488891\r\n",
      "test/church/images/00010.png,church,church,0.9966184119296846;-0.017631121037098282;-0.08025512179125256;0.01232701722294375;0.997736023646746;-0.06611256888039735;0.08123906479729341;0.06489969713762328;0.9945794305445422,-0.42424147848806637;0.8355714070588737;4.249006751343074\r\n",
      "test/church/images/00039.png,church,church,0.5901613385157255;0.24342279180384635;0.7697109450642811;-0.3344742292767577;0.9414995171120779;-0.041299506382559525;-0.7347357242376652;-0.23307510314949004;0.6370552659077613,-1.9093234348324595;0.6311854179493754;1.6417133672835647\r\n",
      "test/church/images/00011.png,church,church,0.9996733485188963;0.01242497944655005;0.02233419232410159;-0.012669958086066597;0.9998607464353482;0.01086093457514942;-0.022196135319340282;-0.011140360115416796;0.9996915644104357,0.3343485701558346;0.9878907126409804;3.651859908968536\r\n",
      "test/church/images/00013.png,church,church,0.9948954144093876;0.019769421051584177;0.09895597191912306;-0.008563390633954488;0.9936249290288152;-0.11241071458420832;-0.10054741532244074;0.11098950582720524;0.9887221788086932,0.6249591391843917;0.5262300313319154;3.7179003593056534\r\n",
      "test/church/images/00006.png,church,church,0.9729986764789417;-0.0723875465565756;0.21916573334749967;0.0870353279801959;0.9945199627880902;-0.05792145802166449;-0.21377190473401186;0.07543266347872993;0.9739668813809313,0.210403291342847;0.9079003317818908;4.6577200548405\r\n",
      "test/church/images/00012.png,church,church,0.9987237737165808;0.007531959535161919;-0.04994089905951444;-0.010295096201223507;0.9984164930473967;-0.05530386428763598;0.04944527083068188;0.05574743040265075;0.9972198299251686,0.27276037655165974;0.42636509756214913;2.3303251962851452\r\n",
      "test/church/images/00029.png,church,church,0.998446417346314;-0.006367002754325135;0.05535533365662723;0.005354805137776792;0.9998160975359119;0.01841459122533622;-0.055462399427418846;-0.018089565610765964;0.9982968946489652,-0.8197663434236774;0.3146887074080224;1.5318372624323306\r\n",
      "test/church/images/00001.png,church,church,0.949342625977007;0.1049760856936379;0.2961901415231819;-0.12199127273442448;0.9917445236017256;0.03950859757852229;-0.28959749287539727;-0.07363980810966543;0.9543115166348229,1.1626344600789669;1.428939422211719;4.448547456183574\r\n",
      "test/church/images/00098.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00072.png,church,church,0.7437452898341849;-0.17660086559454535;-0.6447131750792143;0.32859825701053946;0.9364853460068409;0.12254950919560245;0.5821220914370968;-0.3029972458384811;0.7545373017122449,3.2908894797177624;0.2587580824990138;0.3044247734783932\r\n",
      "test/church/images/00066.png,church,church,0.5382395976350074;-0.23306655258591327;-0.8099247604582929;0.34767498305472605;0.9368229480210384;-0.03853401379387636;0.7677370915200081;-0.26085004528826217;0.585266616319016,4.575082074416128;0.3110740474833159;0.7423574592541503\r\n",
      "test/church/images/00104.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00058.png,church,church,0.9622987635486998;0.18667846156748502;0.19781870907333465;0.02111829218455409;0.6738141741014821;-0.7385989957447446;-0.27117357431698874;0.7149304936612612;0.6444682163034994,0.4415565915750994;0.036855072243636065;0.2784171218924586\r\n",
      "test/church/images/00059.png,church,church,-0.1518601093790466;-0.2665270151609761;-0.9517887672003518;0.5676253621379868;0.7648126134625687;-0.30473449845115175;0.80916003079558;-0.5865364579273282;0.03514296636540548,5.681871488083751;2.4778785206362954;3.92354173596949\r\n",
      "test/church/images/00111.png,church,church,0.8679188615662046;0.17936126545439138;0.4631915221506355;-0.26794327935236106;0.9542737970031221;0.13254402816855512;-0.4182382679616081;-0.23914651744870438;0.8762908731720481,-3.3648062712103144;0.7467548586578054;1.0692680054212755\r\n",
      "test/church/images/00061.png,church,church,0.08960927439914002;0.3177772931129;0.9439214850417897;-0.41945422728446663;0.8716280470021958;-0.2536192005592814;-0.9033428635655737;-0.373205224589603;0.21139898576902716,-5.403045682324934;2.068608185743844;3.88598891919419\r\n",
      "test/church/images/00060.png,church,church,0.9708888123849755;-0.0474337041076373;-0.23478704755654867;0.03877949908422693;0.9983921510987205;-0.04134323493929958;0.2363706082330635;0.031034760174662075;0.9711672251597213,0.5323766996518895;0.35755344976671893;0.9824995412151598\r\n",
      "test/church/images/00074.png,church,church,0.8526163045870626;-0.1637868687807556;-0.49620489595256606;0.154723945179567;0.9861552902369649;-0.0596510211624062;0.49910513716087085;-0.025915345890862116;0.8661538298171898,3.335791606855215;0.07875318992590141;-0.06773741428542536\r\n",
      "test/church/images/00102.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00076.png,church,church,0.45745080864710785;-0.24289091363514798;-0.8554196407270471;0.30802342600254434;0.9456988277180688;-0.10380412461133477;0.8341824301085239;-0.21600400766225042;0.5074267848390389,4.393400053560518;1.0453288706919595;2.8049206346161735\r\n",
      "test/church/images/00063.png,church,church,0.8894943772634478;-0.1552507184598733;-0.42976385054399857;0.3979167148442654;0.725533789350888;0.5614828657739155;0.22463757669690698;-0.6704460716029548;0.7071350820090224,0.31899528844777564;0.066470585024719;-0.01241942852024212\r\n"
     ]
    }
   ],
   "source": [
    "!cat submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 2058261,
     "sourceId": 3414836,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2458397,
     "sourceId": 4165268,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3117886,
     "sourceId": 5373920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5080154,
     "sourceId": 8510477,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5106124,
     "sourceId": 8546179,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5106205,
     "sourceId": 8547652,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170475544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170565695,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174129945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175679956,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175684111,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2742,
     "sourceId": 3840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2747,
     "sourceId": 3846,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 24267,
     "sourceId": 28823,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1172.585993,
   "end_time": "2024-06-04T00:28:22.291706",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-04T00:08:49.705713",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
